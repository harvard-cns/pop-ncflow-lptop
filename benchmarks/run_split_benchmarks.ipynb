{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark_consts import get_problems, get_args_and_problems, print_, PATH_FORM_HYPERPARAMS\n",
    "\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import traceback\n",
    "import argparse\n",
    "import random\n",
    "import math\n",
    "import copy\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from lib.algorithms import PathFormulation\n",
    "from lib.problem import Problem\n",
    "from lib.algorithms.abstract_formulation import Objective\n",
    "from lib.graph_utils import compute_in_or_out_flow, path_to_edge_list, assert_flow_conservation, check_feasibility\n",
    "from collections import defaultdict\n",
    "import ncflow\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from datetime import datetime\n",
    "startTime = datetime.now()\n",
    "\n",
    "TOP_DIR = 'path-form-logs'\n",
    "HEADERS = [\n",
    "    'problem', 'num_nodes', 'num_edges', 'traffic_seed', 'scale_factor',\n",
    "    'tm_model', 'num_commodities', 'total_demand', 'algo', 'num_paths',\n",
    "    'edge_disjoint', 'dist_metric', 'total_flow', 'runtime'\n",
    "]\n",
    "PLACEHOLDER = ','.join('{}' for _ in HEADERS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subproblem_list: list of lists, one for each subproblem, containing assigned entities\n",
    "# input_set_dims: dictionary mapping entity to its dimensions\n",
    "# calculate mean value of every dimension of each subproblem and return\n",
    "def check_dims(subproblem_list, input_set_dims):\n",
    "    \n",
    "    num_subproblems = len(subproblem_list)\n",
    "    num_dimensions = len(list(input_set_dims.values())[0])\n",
    "    print(\"checking split of \" + str(num_dimensions) + \" dimensions over \" + \n",
    "          str(num_subproblems) + \" subproblems\")\n",
    "    \n",
    "    subproblem_dim_sums = [np.zeros(num_dimensions) for _ in range(num_subproblems)]\n",
    "    \n",
    "    for k in range(num_subproblems):\n",
    "        subproblem_entities = subproblem_list[k]\n",
    "        \n",
    "        for entity in subproblem_entities:\n",
    "            entity_dims = input_set_dims[entity]\n",
    "            subproblem_dim_sums[k] = np.add(np.asarray(entity_dims), subproblem_dim_sums[k])\n",
    "        subproblem_dim_sums[k] = subproblem_dim_sums[k]/len(subproblem_entities)\n",
    "        print(\"subproblem \" + str(k) + \": \" + str(subproblem_dim_sums[k]))\n",
    "    return subproblem_dim_sums\n",
    "\n",
    "# estimate of covariance with new entity included\n",
    "def calc_cov_online(d1, d2, current_cov, num_entity, mean_d1, mean_d2):\n",
    "\n",
    "    new_mean_d1 = (mean_d1*num_entity + d1)/(num_entity+1) \n",
    "    new_mean_d2 = (mean_d2*num_entity + d2)/(num_entity+1) \n",
    "    \n",
    "    new_cov = current_cov*(num_entity-1) + \\\n",
    "              (num_entity/(num_entity-1))*(d1 - new_mean_d1)*(d2 - new_mean_d2)\n",
    "    new_cov = new_cov/num_entity\n",
    "    return new_cov\n",
    "\n",
    "# calculate the change in MSE between subproblem inputs and all inputs covariance\n",
    "\n",
    "def calc_dist_cov_change(input_set_dims, new_entity, origin_dist_covs, \n",
    "                         num_entity_mean, current_covs):\n",
    "    num_dims = len(new_entity)\n",
    "    #print(\"old covs: \" + str(current_covs))\n",
    "\n",
    "    new_covs = np.zeros((num_dims,num_dims))\n",
    "    if num_entity_mean[0][0] > 0:\n",
    "        # calculate it from scratch for the first 50 elements\n",
    "        if (num_entity_mean[0][0] < 50):\n",
    "            input_set_dims_new = copy.deepcopy(input_set_dims)\n",
    "            for d in range(num_dims):\n",
    "                input_set_dims_new[d] += [new_entity[d]]\n",
    "            new_covs = np.cov(input_set_dims_new)\n",
    "        # use online update estimate\n",
    "        else:\n",
    "            for d1 in range(num_dims):\n",
    "                # covariance matrix is symmetric, so compute only upper triangle\n",
    "                for d2 in range(d1, num_dims):\n",
    "                    updated_cov = calc_cov_online(new_entity[d1], new_entity[d2], \n",
    "                                                  current_covs[d1,d2], num_entity_mean[d1][0],\n",
    "                                                  num_entity_mean[d1][1], num_entity_mean[d2][1])\n",
    "                    new_covs[d1,d2] = updated_cov\n",
    "                    new_covs[d2,d1] = updated_cov\n",
    "    \n",
    "    #print(\"new cov: \" + str(new_covs))\n",
    "    \n",
    "    old_mse = ((current_covs - origin_dist_covs)**2).mean(axis=None)\n",
    "    new_mse = ((new_covs - origin_dist_covs)**2).mean(axis=None)\n",
    "    \n",
    "    dist_diff = old_mse - new_mse\n",
    "    return dist_diff, new_covs\n",
    "\n",
    "# Compute the change in distance (2-norm of dimensional means)\n",
    "# from the original problem inputs when adding new entity\n",
    "# TODO: better distance metric that considers covariance?\n",
    "def calc_dist_mean_change(input_set_dims, new_entity, origin_dist, num_entity_mean):\n",
    "    num_dims = len(new_entity)\n",
    "    \n",
    "    sq_sum_distance = 0\n",
    "    sq_sum_distance_new = 0\n",
    "    for d in range(num_dims):\n",
    "        if len(input_set_dims[d]) == 0:\n",
    "            current_mean = 0\n",
    "        else:\n",
    "            #current_mean = np.mean(input_set_dims[d])\n",
    "            current_mean = num_entity_mean[d][1]\n",
    "            \n",
    "        #new_mean = np.mean(input_set_dims[d]+[new_entity[d]])\n",
    "        new_mean = (num_entity_mean[d][1]*num_entity_mean[d][0] + new_entity[d]) \\\n",
    "                    /(num_entity_mean[d][0]+1)\n",
    "        sq_sum_distance_new += ((new_mean - origin_dist[d])/origin_dist[d])**2\n",
    "        sq_sum_distance += ((current_mean - origin_dist[d])/origin_dist[d])**2\n",
    "    \n",
    "    return math.sqrt(sq_sum_distance) - math.sqrt(sq_sum_distance_new)\n",
    "        \n",
    "\n",
    "# input_dict: keys are entities and values are (ordered) list of entity dimensions, \n",
    "# k: number of subproblems\n",
    "def split_generic(input_dict, k, verbose=False):\n",
    "    \n",
    "    num_inputs = len(input_dict)\n",
    "    num_dimensions = len(list(input_dict.values())[0])\n",
    "    \n",
    "    # original_dist_dict: keys are dimension indices (0..d), values are frequency\n",
    "    original_dist_means_dict = {}\n",
    "    original_dist_inputs_by_dim = []\n",
    "    for d in range(num_dimensions):\n",
    "        inputs = [val[d] for val in input_dict.values()]\n",
    "        original_dist_inputs_by_dim.append(inputs)\n",
    "        sum_d = sum(inputs)\n",
    "        original_dist_means_dict[d] = sum_d/(num_inputs*1.0)\n",
    "    \n",
    "    original_dist_cov = np.cov(original_dist_inputs_by_dim)\n",
    "    \n",
    "    # subproblem_dim_lists has k lists, one for each subproblem. Within each list are d lists,\n",
    "    # each containing the value of a dimension for each entity assigned to that subproblem\n",
    "    subproblem_dim_lists =  [[[] for _ in range(num_dimensions)] for _ in range(k)]\n",
    "    \n",
    "    # subproblem_entity_assignments is a list of lists\n",
    "    subproblem_entity_assignments = [[] for _ in range(k)]\n",
    "    \n",
    "    # Assign each entity to the sub-problem that would have their distance from the\n",
    "    # original distribution shrink the most.\n",
    "    num_assigned = 0\n",
    "    subproblem_num_entity_means = [[[0,0] for _ in range(num_dimensions)] for _ in range(k)]\n",
    "    subproblem_covs = [np.zeros((num_dimensions,num_dimensions)) for _ in range(k)]\n",
    "    for entity, dims in input_dict.items():\n",
    "        if num_assigned % 1000 == 0:\n",
    "            print(\"Assigned \" + str(num_assigned) + \" entities\")\n",
    "        max_dist_change = -np.inf\n",
    "        max_dist_sp = 0\n",
    "        updated_cov = None\n",
    "        for i in range(k):\n",
    "            # skip those that have more than equal share of currently assigned entities\n",
    "            if len(subproblem_entity_assignments[i]) > num_assigned/(k*1.0):\n",
    "                continue\n",
    "                \n",
    "            #dist_change = calc_dist_mean_change(subproblem_dim_lists[i], dims, \n",
    "            #                               original_dist_means_dict, subproblem_num_entity_means[i])\n",
    "            dist_change, new_cov = calc_dist_cov_change(subproblem_dim_lists[i], dims, \n",
    "                                           original_dist_cov, subproblem_num_entity_means[i],\n",
    "                                                       subproblem_covs[i])\n",
    "            #print(\"subproblem \" + str(i) + \", dist change: \" + str(dist_change))\n",
    "            if dist_change >= max_dist_change:\n",
    "                max_dist_change = dist_change\n",
    "                max_dist_sp = i\n",
    "                #updated_cov = new_cov\n",
    "                \n",
    "        subproblem_entity_assignments[max_dist_sp].append(entity)\n",
    "        #subproblem_covs[max_dist_sp] = new_cov\n",
    "        \n",
    "        # update means to reflect entity assignment\n",
    "        for d in range(num_dimensions):\n",
    "            subproblem_dim_lists[max_dist_sp][d].append(dims[d])\n",
    "            \n",
    "            num_entity = subproblem_num_entity_means[max_dist_sp][d][0]\n",
    "            dim_mean = subproblem_num_entity_means[max_dist_sp][d][0]\n",
    "            subproblem_num_entity_means[max_dist_sp][d][0] += 1\n",
    "            subproblem_num_entity_means[max_dist_sp][d][1] = (dim_mean*num_entity + dims[d])/(num_entity+1)\n",
    "        \n",
    "        num_assigned += 1\n",
    "        \n",
    "        if verbose:\n",
    "            print(subproblem_dim_lists)\n",
    "            print(subproblem_entity_assignments)\n",
    "            print('\\n')\n",
    "    return subproblem_entity_assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigned 0 entities\n",
      "[[[], [], []], [[1], [2], [3]]]\n",
      "[[], ['A']]\n",
      "\n",
      "\n",
      "[[[4], [5], [6]], [[1], [2], [3]]]\n",
      "[['B'], ['A']]\n",
      "\n",
      "\n",
      "[[[4], [5], [6]], [[1, 7], [2, 8], [3, 9]]]\n",
      "[['B'], ['A', 'C']]\n",
      "\n",
      "\n",
      "[[[4, 1], [5, 2], [6, 3]], [[1, 7], [2, 8], [3, 9]]]\n",
      "[['B', 'D'], ['A', 'C']]\n",
      "\n",
      "\n",
      "[[[4, 1, 4], [5, 2, 5], [6, 3, 6]], [[1, 7], [2, 8], [3, 9]]]\n",
      "[['B', 'D', 'E'], ['A', 'C']]\n",
      "\n",
      "\n",
      "[[[4, 1, 4], [5, 2, 5], [6, 3, 6]], [[1, 7, 7], [2, 8, 8], [3, 9, 9]]]\n",
      "[['B', 'D', 'E'], ['A', 'C', 'F']]\n",
      "\n",
      "\n",
      "checking split of 3 dimensions over 2 subproblems\n",
      "subproblem 0: [3. 4. 5.]\n",
      "subproblem 1: [5. 6. 7.]\n",
      "Loading paths from pickle file /lfs/1/fiodar/ncflow/topologies/paths/path-form/Ion.graphml-4-paths_edge-disjoint-True_dist-metric-inv-cap-dict.pkl\n",
      "paths_dict size: 15500\n",
      "creating subproblem list...\n",
      "Assigned 0 entities\n",
      "Assigned 1000 entities\n",
      "Assigned 2000 entities\n",
      "Assigned 3000 entities\n",
      "Assigned 4000 entities\n",
      "Assigned 5000 entities\n",
      "Assigned 6000 entities\n",
      "Assigned 7000 entities\n",
      "Assigned 8000 entities\n",
      "Assigned 9000 entities\n",
      "Assigned 10000 entities\n",
      "Assigned 11000 entities\n",
      "Assigned 12000 entities\n",
      "Assigned 13000 entities\n",
      "Assigned 14000 entities\n",
      "Assigned 15000 entities\n"
     ]
    }
   ],
   "source": [
    "original_dist = [2,5]\n",
    "A = [[1,2,3], [4,5,6]]\n",
    "B = [2,5]\n",
    "C = [3,6]\n",
    "#print(calc_dist(A,B,original_dist))\n",
    "#print(calc_dist(A,C,original_dist))\n",
    "\n",
    "input_dict = {'A': [1,2,3],\n",
    "              'B': [4,5,6],\n",
    "              'C': [7,8,9],\n",
    "              'D': [1,2,3],\n",
    "              'E': [4,5,6],\n",
    "              'F': [7,8,9]}\n",
    "subproblem_list = split_generic(input_dict,2, verbose=True)\n",
    "check_dims(subproblem_list, input_dict)\n",
    "\n",
    "\n",
    "topo_fname = \"../topologies/topology-zoo/Ion.graphml\"#GtsCe.graphml\"\n",
    "#tm_fname = \"../traffic-matrices/uniform/GtsCe.graphml_uniform_1475504323_64.0_0.05_traffic-matrix.pkl\"\n",
    "tm_fname = \"../traffic-matrices/uniform/Ion.graphml_uniform_1545787193_64.0_0.15_traffic-matrix.pkl\"\n",
    "num_paths, edge_disjoint, dist_metric = PATH_FORM_HYPERPARAMS\n",
    "    \n",
    "pf_original = PathFormulation.new_max_flow(\n",
    "    num_paths,\n",
    "    edge_disjoint=edge_disjoint,\n",
    "    dist_metric=dist_metric)\n",
    "with open('path-form.csv', 'a') as results:\n",
    "    print_(','.join(HEADERS), file=results)\n",
    "    \n",
    "    problem = Problem.from_file(topo_fname, tm_fname)\n",
    "\n",
    "    paths_dict = pf_original.get_paths(problem)\n",
    "    com_list = problem.commodity_list\n",
    "    num_edges = len(problem.G.edges)\n",
    "    enum_edges_dict = {}\n",
    "    for i, edge in enumerate(problem.G.edges):\n",
    "        enum_edges_dict[edge] = i\n",
    "\n",
    "    # create dictionary of all edges used by each commodity\n",
    "    com_path_edges_dict = defaultdict(list)\n",
    "    min_demand = np.inf\n",
    "    max_demand = 0\n",
    "    for k, (source, target, demand) in com_list:\n",
    "        paths_array = paths_dict[(source, target)]\n",
    "        if min_demand > demand:\n",
    "            min_demand = demand\n",
    "        if max_demand < demand:\n",
    "            max_demand = demand\n",
    "            \n",
    "        for path in paths_array:\n",
    "            com_path_edges_dict[(k, source, target, demand)] += list(path_to_edge_list(path))\n",
    "\n",
    "    com_path_edges_onehot_dict = defaultdict(list)\n",
    "    for (k, source, target, demand), edge_list in com_path_edges_dict.items():\n",
    "        onehot_edge = [0]*num_edges\n",
    "        for edge in edge_list:\n",
    "            edge_i = enum_edges_dict[edge]\n",
    "            onehot_edge[edge_i] = 1\n",
    "        # add in normalized demand as a dimension\n",
    "        com_path_edges_onehot_dict[k] = onehot_edge + [(demand - min_demand)/(max_demand-min_demand)]\n",
    "        \n",
    "    print(\"creating subproblem list...\")\n",
    "    subproblem_list_meansplit = split_generic(com_path_edges_onehot_dict, 2, verbose=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking dims...\n",
      "checking split of 293 dimensions over 2 subproblems\n",
      "subproblem 0: [0.13058065 0.13225806 0.064      0.06090323 0.07122581 0.07109677\n",
      " 0.07058065 0.00477419 0.05303226 0.00464516 0.01122581 0.008\n",
      " 0.04980645 0.04916129 0.008      0.008      0.21045161 0.21070968\n",
      " 0.13109677 0.12903226 0.18322581 0.18825806 0.18464516 0.18980645\n",
      " 0.008      0.19354839 0.20206452 0.02516129 0.02374194 0.03096774\n",
      " 0.03058065 0.02503226 0.02374194 0.0196129  0.02129032 0.18309677\n",
      " 0.20645161 0.12696774 0.004      0.19793548 0.20012903 0.04941935\n",
      " 0.05535484 0.08451613 0.04374194 0.12180645 0.0083871  0.00154839\n",
      " 0.0076129  0.10877419 0.108      0.05419355 0.0476129  0.13251613\n",
      " 0.12851613 0.11096774 0.20593548 0.14709677 0.11135484 0.10503226\n",
      " 0.13496774 0.11380645 0.07587097 0.01922581 0.06929032 0.00787097\n",
      " 0.07574194 0.09419355 0.02012903 0.02283871 0.02064516 0.02129032\n",
      " 0.01896774 0.02245161 0.02090323 0.06541935 0.06670968 0.06812903\n",
      " 0.00980645 0.06       0.044      0.0516129  0.04529032 0.05225806\n",
      " 0.11870968 0.13122581 0.11587097 0.12941935 0.12451613 0.1283871\n",
      " 0.13006452 0.12129032 0.05019355 0.04825806 0.04567742 0.04890323\n",
      " 0.11729032 0.13212903 0.052      0.11432258 0.11883871 0.04696774\n",
      " 0.04658065 0.01483871 0.01729032 0.01574194 0.016      0.01832258\n",
      " 0.14283871 0.03896774 0.09406452 0.13858065 0.13987097 0.008\n",
      " 0.10077419 0.10322581 0.10541935 0.10258065 0.008      0.044\n",
      " 0.00077419 0.04270968 0.05290323 0.05819355 0.0563871  0.05148387\n",
      " 0.05793548 0.05432258 0.18941935 0.05432258 0.20812903 0.2\n",
      " 0.23212903 0.05419355 0.09406452 0.09896774 0.05651613 0.05225806\n",
      " 0.05135484 0.05819355 0.14709677 0.13754839 0.13483871 0.05303226\n",
      " 0.112      0.14916129 0.15432258 0.14916129 0.14606452 0.13767742\n",
      " 0.16954839 0.04141935 0.13587097 0.04425806 0.04245161 0.06270968\n",
      " 0.06335484 0.06348387 0.06245161 0.008      0.23574194 0.23380645\n",
      " 0.1636129  0.15729032 0.16890323 0.0083871  0.23419355 0.18980645\n",
      " 0.22335484 0.04296774 0.04309677 0.004      0.10219355 0.04412903\n",
      " 0.12929032 0.09354839 0.0916129  0.09251613 0.09406452 0.12374194\n",
      " 0.00658065 0.12825806 0.08722581 0.00580645 0.09109677 0.13083871\n",
      " 0.12980645 0.12812903 0.12670968 0.13716129 0.13574194 0.008\n",
      " 0.008      0.13729032 0.13625806 0.13677419 0.14593548 0.08864516\n",
      " 0.07780645 0.13509677 0.15729032 0.0243871  0.13883871 0.13277419\n",
      " 0.08683871 0.07987097 0.20709677 0.20941935 0.04335484 0.01754839\n",
      " 0.03264516 0.12851613 0.13406452 0.00877419 0.04258065 0.04283871\n",
      " 0.01083871 0.13690323 0.14129032 0.02206452 0.05483871 0.05483871\n",
      " 0.05729032 0.05109677 0.07109677 0.07316129 0.20722581 0.13664516\n",
      " 0.11935484 0.0083871  0.00864516 0.008      0.05277419 0.05845161\n",
      " 0.00993548 0.008      0.07780645 0.06941935 0.13651613 0.09187097\n",
      " 0.00709677 0.10064516 0.21290323 0.11845161 0.09729032 0.09496774\n",
      " 0.09832258 0.14258065 0.10387097 0.09380645 0.16309677 0.22464516\n",
      " 0.12503226 0.02103226 0.03419355 0.15496774 0.12374194 0.02103226\n",
      " 0.18606452 0.17174194 0.12993548 0.16580645 0.04580645 0.06077419\n",
      " 0.03393548 0.04632258 0.15264516 0.15793548 0.11122581 0.11858065\n",
      " 0.06451613 0.23574194 0.15187097 0.09032258 0.08567742 0.08941935\n",
      " 0.08658065 0.08993548 0.08477419 0.08606452 0.03393548 0.02180645\n",
      " 0.03651613 0.02348387 0.02180645 0.02232258 0.03909677 0.01135484\n",
      " 0.02967742 0.23290323 0.09935484 0.15690323 0.502718  ]\n",
      "subproblem 1: [0.12954839 0.12619355 0.06309677 0.06451613 0.07483871 0.07509677\n",
      " 0.07548387 0.01122581 0.05032258 0.01135484 0.00477419 0.008\n",
      " 0.05354839 0.05329032 0.008      0.008      0.21070968 0.21277419\n",
      " 0.12916129 0.12954839 0.19587097 0.18851613 0.19458065 0.18709677\n",
      " 0.008      0.20490323 0.19329032 0.02270968 0.024      0.0316129\n",
      " 0.03225806 0.0243871  0.02425806 0.02825806 0.02670968 0.19625806\n",
      " 0.18877419 0.12503226 0.012      0.20064516 0.19741935 0.04774194\n",
      " 0.04051613 0.0843871  0.04309677 0.12167742 0.0083871  0.00129032\n",
      " 0.00877419 0.10993548 0.10916129 0.03909677 0.04670968 0.12580645\n",
      " 0.13148387 0.10632258 0.21574194 0.14296774 0.10722581 0.112\n",
      " 0.12322581 0.10464516 0.08451613 0.02554839 0.06193548 0.00929032\n",
      " 0.07367742 0.0916129  0.02606452 0.02335484 0.02412903 0.02348387\n",
      " 0.0243871  0.02232258 0.02245161 0.06580645 0.0643871  0.06296774\n",
      " 0.00619355 0.05703226 0.04916129 0.04051613 0.05045161 0.04219355\n",
      " 0.12967742 0.11664516 0.13122581 0.11780645 0.12670968 0.12219355\n",
      " 0.11909677 0.12851613 0.04322581 0.04670968 0.04632258 0.04464516\n",
      " 0.12916129 0.11470968 0.04283871 0.13174194 0.10748387 0.04348387\n",
      " 0.04296774 0.0156129  0.01406452 0.0156129  0.01651613 0.01419355\n",
      " 0.13664516 0.03987097 0.09277419 0.14090323 0.14051613 0.008\n",
      " 0.09935484 0.10232258 0.10051613 0.09793548 0.008      0.04309677\n",
      " 0.00206452 0.04658065 0.05858065 0.05341935 0.05509677 0.05987097\n",
      " 0.05341935 0.05690323 0.18722581 0.05716129 0.2156129  0.20077419\n",
      " 0.23664516 0.05909677 0.09716129 0.09716129 0.05677419 0.0596129\n",
      " 0.06025806 0.05354839 0.14283871 0.1483871  0.12554839 0.04425806\n",
      " 0.11458065 0.148      0.14387097 0.14812903 0.14374194 0.1483871\n",
      " 0.17058065 0.04696774 0.13690323 0.04348387 0.04529032 0.06554839\n",
      " 0.06503226 0.06348387 0.0643871  0.008      0.24       0.23909677\n",
      " 0.16890323 0.16580645 0.17187097 0.0076129  0.23883871 0.18258065\n",
      " 0.22993548 0.04554839 0.04387097 0.012      0.09754839 0.04374194\n",
      " 0.12864516 0.09096774 0.09432258 0.09458065 0.0916129  0.11974194\n",
      " 0.00980645 0.1276129  0.08141935 0.01096774 0.09367742 0.12619355\n",
      " 0.1283871  0.12774194 0.13032258 0.13354839 0.13587097 0.008\n",
      " 0.008      0.13548387 0.13535484 0.14941935 0.144      0.07135484\n",
      " 0.08270968 0.13935484 0.15677419 0.02232258 0.13974194 0.14154839\n",
      " 0.07329032 0.08077419 0.21651613 0.21187097 0.04735484 0.01290323\n",
      " 0.03212903 0.12967742 0.13664516 0.00722581 0.04451613 0.04477419\n",
      " 0.00516129 0.14916129 0.13716129 0.02141935 0.0563871  0.05664516\n",
      " 0.05458065 0.06064516 0.07509677 0.07341935 0.21380645 0.13548387\n",
      " 0.12283871 0.00877419 0.00787097 0.008      0.04967742 0.05858065\n",
      " 0.00606452 0.008      0.0716129  0.07716129 0.1356129  0.09393548\n",
      " 0.00941935 0.09535484 0.21122581 0.12129032 0.09883871 0.10103226\n",
      " 0.09780645 0.13780645 0.10245161 0.09651613 0.16903226 0.22606452\n",
      " 0.12645161 0.02670968 0.02825806 0.14335484 0.12167742 0.0283871\n",
      " 0.18683871 0.15974194 0.11612903 0.16619355 0.04658065 0.06477419\n",
      " 0.02903226 0.04722581 0.15741935 0.15625806 0.11419355 0.10890323\n",
      " 0.06529032 0.23987097 0.15832258 0.08270968 0.0876129  0.08348387\n",
      " 0.08658065 0.08283871 0.07548387 0.08696774 0.03870968 0.02193548\n",
      " 0.036      0.02296774 0.02477419 0.02425806 0.03948387 0.00464516\n",
      " 0.03535484 0.23909677 0.09767742 0.16658065 0.49956431]\n",
      "[ 0.00103226  0.00606452  0.00090323 -0.0036129  -0.0036129  -0.004\n",
      " -0.00490323 -0.00645161  0.00270968 -0.00670968  0.00645161  0.\n",
      " -0.00374194 -0.00412903  0.          0.         -0.00025806 -0.00206452\n",
      "  0.00193548 -0.00051613 -0.01264516 -0.00025806 -0.00993548  0.00270968\n",
      "  0.         -0.01135484  0.00877419  0.00245161 -0.00025806 -0.00064516\n",
      " -0.00167742  0.00064516 -0.00051613 -0.00864516 -0.00541935 -0.01316129\n",
      "  0.01767742  0.00193548 -0.008      -0.00270968  0.00270968  0.00167742\n",
      "  0.01483871  0.00012903  0.00064516  0.00012903  0.          0.00025806\n",
      " -0.00116129 -0.00116129 -0.00116129  0.01509677  0.00090323  0.00670968\n",
      " -0.00296774  0.00464516 -0.00980645  0.00412903  0.00412903 -0.00696774\n",
      "  0.01174194  0.00916129 -0.00864516 -0.00632258  0.00735484 -0.00141935\n",
      "  0.00206452  0.00258065 -0.00593548 -0.00051613 -0.00348387 -0.00219355\n",
      " -0.00541935  0.00012903 -0.00154839 -0.0003871   0.00232258  0.00516129\n",
      "  0.0036129   0.00296774 -0.00516129  0.01109677 -0.00516129  0.01006452\n",
      " -0.01096774  0.01458065 -0.01535484  0.0116129  -0.00219355  0.00619355\n",
      "  0.01096774 -0.00722581  0.00696774  0.00154839 -0.00064516  0.00425806\n",
      " -0.01187097  0.01741935  0.00916129 -0.01741935  0.01135484  0.00348387\n",
      "  0.0036129  -0.00077419  0.00322581  0.00012903 -0.00051613  0.00412903\n",
      "  0.00619355 -0.00090323  0.00129032 -0.00232258 -0.00064516  0.\n",
      "  0.00141935  0.00090323  0.00490323  0.00464516  0.          0.00090323\n",
      " -0.00129032 -0.00387097 -0.00567742  0.00477419  0.00129032 -0.0083871\n",
      "  0.00451613 -0.00258065  0.00219355 -0.00283871 -0.00748387 -0.00077419\n",
      " -0.00451613 -0.00490323 -0.00309677  0.00180645 -0.00025806 -0.00735484\n",
      " -0.00890323  0.00464516  0.00425806 -0.01083871  0.00929032  0.00877419\n",
      " -0.00258065  0.00116129  0.01045161  0.00103226  0.00232258 -0.01070968\n",
      " -0.00103226 -0.00554839 -0.00103226  0.00077419 -0.00283871 -0.00283871\n",
      " -0.00167742  0.         -0.00193548  0.         -0.00425806 -0.00529032\n",
      " -0.00529032 -0.00851613 -0.00296774  0.00077419 -0.00464516  0.00722581\n",
      " -0.00658065 -0.00258065 -0.00077419 -0.008       0.00464516  0.0003871\n",
      "  0.00064516  0.00258065 -0.00270968 -0.00206452  0.00245161  0.004\n",
      " -0.00322581  0.00064516  0.00580645 -0.00516129 -0.00258065  0.00464516\n",
      "  0.00141935  0.0003871  -0.0036129   0.0036129  -0.00012903  0.\n",
      "  0.          0.00180645  0.00090323 -0.01264516  0.00193548  0.01729032\n",
      " -0.00490323 -0.00425806  0.00051613  0.00206452 -0.00090323 -0.00877419\n",
      "  0.01354839 -0.00090323 -0.00941935 -0.00245161 -0.004       0.00464516\n",
      "  0.00051613 -0.00116129 -0.00258065  0.00154839 -0.00193548 -0.00193548\n",
      "  0.00567742 -0.01225806  0.00412903  0.00064516 -0.00154839 -0.00180645\n",
      "  0.00270968 -0.00954839 -0.004      -0.00025806 -0.00658065  0.00116129\n",
      " -0.00348387 -0.0003871   0.00077419  0.          0.00309677 -0.00012903\n",
      "  0.00387097  0.          0.00619355 -0.00774194  0.00090323 -0.00206452\n",
      " -0.00232258  0.00529032  0.00167742 -0.00283871 -0.00154839 -0.00606452\n",
      "  0.00051613  0.00477419  0.00141935 -0.00270968 -0.00593548 -0.00141935\n",
      " -0.00141935 -0.00567742  0.00593548  0.0116129   0.00206452 -0.00735484\n",
      " -0.00077419  0.012       0.01380645 -0.0003871  -0.00077419 -0.004\n",
      "  0.00490323 -0.00090323 -0.00477419  0.00167742 -0.00296774  0.00967742\n",
      " -0.00077419 -0.00412903 -0.00645161  0.0076129  -0.00193548  0.00593548\n",
      "  0.          0.00709677  0.00929032 -0.00090323 -0.00477419 -0.00012903\n",
      "  0.00051613  0.00051613 -0.00296774 -0.00193548 -0.0003871   0.00670968\n",
      " -0.00567742 -0.00619355  0.00167742 -0.00967742  0.00315368]\n",
      "0.004181191700309575\n"
     ]
    }
   ],
   "source": [
    "print(\"checking dims...\")\n",
    "subproblem_dim_means = check_dims(subproblem_list_meansplit, com_path_edges_onehot_dict)\n",
    "print(subproblem_dim_means[0] - subproblem_dim_means[1])\n",
    "print(np.mean(abs(subproblem_dim_means[0] - subproblem_dim_means[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Firas's Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_name, topo_fname, tm_fname = p8\n",
    "prob = Problem.from_file(topo_fname, tm_fname)\n",
    "NUM_PATHS = 4\n",
    "NUM_SUBPROBLEMS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_subproblems(problem_list):\n",
    "    sol_dicts, runtimes, obj_vals = [], [], []\n",
    "    for sub_problem in problem_list:\n",
    "        pf = PathFormulation.get_pf_for_obj(Objective.MAX_FLOW, NUM_PATHS)\n",
    "        pf.solve(sub_problem)\n",
    "        sol_dicts.append(pf.extract_sol_as_dict())\n",
    "        runtimes.append(pf.runtime)\n",
    "        obj_vals.append(pf.obj_val)\n",
    "    #check\n",
    "    return sol_dicts, runtimes, obj_vals\n",
    "\n",
    "def solve_and_check_feasiblity(problem, num_subproblems, num_paths):    \n",
    "    paths_dict = PathFormulation.new_max_flow(num_paths).get_paths(problem)\n",
    "    problem_list = split_problem_smartpath(prob, num_subproblems, paths_dict)\n",
    "    sol_dicts, runtimes, obj_vals = solve_subproblems(problem_list)\n",
    "    check_feasibility(problem, sol_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "solve_and_check_feasiblity(prob, NUM_SUBPROBLEMS, NUM_PATHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End Firas's Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_solution(sol_dicts_all, num_subproblems):\n",
    "    for obj_type in obj_types:\n",
    "        sol_dicts = sol_dicts_all[obj_type]\n",
    "        for p_spec in problems:\n",
    "            problem = Problem.from_file(p_spec[1], p_spec[2a])\n",
    "            com_list = problem.commodity_list\n",
    "            \n",
    "            for n_i, n in enumerate(num_subproblems):\n",
    "                merged_sol_dict = defaultdict(int)\n",
    "                for j in range(n):\n",
    "                    sol_dict = sol_dicts[p_spec][n_i][j]\n",
    "                    total_flow = 0\n",
    "                    for commod_key, flow_list in sol_dict.items():\n",
    "                        assert_flow_conservation(flow_list, commod_key)\n",
    "                        src, target = commod_key[-1][0], commod_key[-1][1]\n",
    "\n",
    "                        flow = compute_in_or_out_flow(flow_list, 0, {commod_key[-1][0]})\n",
    "\n",
    "                        merged_sol_dict[(src,target)] += flow\n",
    "            \n",
    "                frac_demands_satisfied = {commod_key:\n",
    "                                      merged_sol_dict[(commod_key[-1][0],\n",
    "                                                       commod_key[-1][1])] / commod_key[-1][-1]\n",
    "                                      for commod_key in com_list}\n",
    "                for commod_key, frac in frac_demands_satisfied.items():\n",
    "                    if frac > 1:\n",
    "                        print(\"assertion error, demand oversatisfied \"+ str(commod_key) + \" \" + str(frac))\n",
    "                        break\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_all_obj = {}\n",
    "runtimes_all_obj = {}\n",
    "sol_dicts_all_obj = {}\n",
    "for obj_type in obj_types:\n",
    "    \n",
    "    results, runtimes, sol_dicts = benchmark_split(problems, num_subproblems, obj_type, smart=False)\n",
    "    results_all_obj[obj_type] = results\n",
    "    runtimes_all_obj[obj_type] = runtimes\n",
    "    sol_dicts_all_obj[obj_type] = sol_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_all_obj_smart = {}\n",
    "runtimes_all_obj_smart = {}\n",
    "sol_dicts_all_obj_smart = {}\n",
    "for obj_type in obj_types:\n",
    "    \n",
    "    results, runtimes, sol_dicts = benchmark_split(problems, num_subproblems, obj_type, smart=True, generic=False)\n",
    "    results_all_obj_smart[obj_type] = results\n",
    "    runtimes_all_obj_smart[obj_type] = runtimes\n",
    "    sol_dicts_all_obj_smart[obj_type] = sol_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_all_obj_smart_gen = {}\n",
    "runtimes_all_obj_smart_gen = {}\n",
    "sol_dicts_all_obj_smart_gen = {}\n",
    "for obj_type in obj_types:\n",
    "    \n",
    "    results, runtimes, sol_dicts = benchmark_split(problems, num_subproblems, obj_type, smart=True, generic=True)\n",
    "    results_all_obj_smart_gen[obj_type] = results\n",
    "    runtimes_all_obj_smart_gen[obj_type] = runtimes\n",
    "    sol_dicts_all_obj_smart_gen[obj_type] = sol_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smart_results = [results_all_obj_smart, runtimes_all_obj_smart, sol_dicts_all_obj_smart]\n",
    "pickle.dump(smart_results, open(\"results/smart_results.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[results_all_obj_smart, \n",
    "runtimes_all_obj_smart, \n",
    "sol_dicts_all_obj_smart] = pickle.load(open(\"results/smart_results.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run NCFlow on problem set\n",
    "problems_ncflow = [(os.path.basename(p[1]), p[1], p[2]) for p in problems]\n",
    "results_ncflow, runtimes_ncflow = ncflow.benchmark(problems_ncflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['k', 'r', 'b', 'g']\n",
    "linestyles = ['-', '--', ':']\n",
    "#print(results_ncflow)\n",
    "#results_ncflow = results_ncflow[1::2]\n",
    "#print(runtimes_ncflow)\n",
    "def compute_obj_val(obj, problem, obj_vals, sol_dicts):\n",
    "    if obj == 'max_flow':\n",
    "        return sum(obj_vals)\n",
    "    elif obj == 'min_max_link_util':\n",
    "        link_utils_dict = defaultdict(int)\n",
    "        for sol_dict in sol_dicts:\n",
    "            for flow_list in sol_dict.values():\n",
    "                for ((u, v), flow_val) in flow_list: # TODO: is this right? Or is it ((u, v), flow_val)?\n",
    "                    link_utils_dict[(u, v)] += flow_val\n",
    "        \n",
    "        # TODO: is this right? Or is it ((u, v), c_e)?\n",
    "        link_utils = {(u, v): link_utils_dict[(u, v)] / c_e for (u, v, c_e) in problem.G.edges.data('capacity')}\n",
    "        return max(link_utils.values())\n",
    "    \n",
    "# run the benchmarks, plot results\n",
    "def plot_benchmark(results_all, runtimes_all, sol_dicts_all, results_ncflow, runtimes_ncflow,\n",
    "                  results_cspf, runtimes_cspf):\n",
    "    for obj_type in obj_types:\n",
    "        \n",
    "        results = results_all[obj_type]\n",
    "        runtimes = runtimes_all[obj_type]\n",
    "        sol_dicts = sol_dicts_all[obj_type]\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        for p_i, p in enumerate(problems):\n",
    "            total_obj_values = []\n",
    "            for n_i, n in enumerate(num_subproblems):\n",
    "                total_obj_val = compute_obj_val(obj_type, Problem.from_file(p[1], p[2]),\n",
    "                                                results[p][n_i], sol_dicts[p][n_i])\n",
    "        #         sum_val = sum(results[p][n_i])\n",
    "                total_obj_values.append(total_obj_val)\n",
    "            ax.plot(num_subproblems, total_obj_values, marker='*', c=colors[p_i%len(colors)], linestyle=linestyles[p_i%len(linestyles)],\n",
    "                    label=p[0])\n",
    "            #if (obj_type == \"max_flow\"):\n",
    "                #also plot flow achieved by ncflow\n",
    "                #ax.plot(1, results_ncflow[p_i], marker='P', markersize=10, c=colors[p_i%len(colors)], label=p[0]+\"; ncflow\")\n",
    "        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.xlabel('number of subproblems')\n",
    "        plt.ylabel('total flow')\n",
    "        #plt.ylim([0,None])\n",
    "        #plt.savefig('./plots/flow_'+p[0]+'.png', bbox_inches='tight')\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        for p_i, p in enumerate(problems):\n",
    "            total_runtime = []\n",
    "            max_runtime = []\n",
    "            for n_i, n in enumerate(num_subproblems):\n",
    "                sum_val = sum(runtimes[p][n_i])\n",
    "                print(sum_val)\n",
    "                max_time = max(runtimes[p][n_i])\n",
    "                total_runtime.append(sum_val)\n",
    "                max_runtime.append(max_time)\n",
    "            ax.plot(num_subproblems, total_runtime, marker='*', linestyle=linestyles[0], \n",
    "                    c=colors[p_i%len(colors)], label='sum ' + p[0])\n",
    "            ax.plot(num_subproblems, max_runtime, marker='^', linestyle=linestyles[1], \n",
    "                    c=colors[p_i%len(colors)], label='max ' + p[0])\n",
    "            if (obj_type == \"max_flow\"):\n",
    "                #also plot runtime of ncflow\n",
    "                ax.plot(1, runtimes_ncflow[p_i], marker='P', markersize=10, c=colors[p_i%len(colors)], label=p[0]+\"; ncflow\")\n",
    "        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.xlabel('number of subproblems')\n",
    "        plt.ylabel('execution time (seconds)')\n",
    "        plt.ylim([0,None])\n",
    "        #plt.savefig('./plots/runtime_'+p[0]+'.png', bbox_inches='tight')\n",
    "\n",
    "        # plot performance vs runtime.\n",
    "        fig, ax = plt.subplots()\n",
    "        for p_i, p in enumerate(problems):\n",
    "            total_obj_values = []\n",
    "            total_runtime = []\n",
    "            max_runtime = []\n",
    "            for n_i, n in enumerate(num_subproblems):\n",
    "                total_obj_val = compute_obj_val(obj_type, Problem.from_file(p[1], p[2]),\n",
    "                                                results[p][n_i], sol_dicts[p][n_i])\n",
    "                total_obj_values.append(total_obj_val)\n",
    "\n",
    "                sum_val = sum(runtimes[p][n_i])\n",
    "                max_time = max(runtimes[p][n_i])\n",
    "                total_runtime.append(sum_val)\n",
    "                max_runtime.append(max_time)\n",
    "            print(max_runtime)\n",
    "            print(total_obj_values)\n",
    "            ax.scatter(max_runtime, total_obj_values, marker='*', c=colors[p_i%len(colors)], label=p[0])\n",
    "            for n_i, n in enumerate(num_subproblems):\n",
    "                ax.annotate(n, (max_runtime[n_i], total_obj_values[n_i]))\n",
    "            if (obj_type == \"max_flow\"):\n",
    "                #also plot runtime of ncflow\n",
    "                ax.plot(runtimes_ncflow[p_i], results_ncflow[p_i], marker='P', markersize=10, c=colors[p_i%len(colors)], label=p[0]+\"; ncflow\")\n",
    "                ax.plot(runtimes_cspf[p_i], results_cspf[p_i], marker='x', markersize=10, c=colors[p_i%len(colors)], label=p[0]+\"; CSPF\")\n",
    "        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.xlabel('execution time (seconds)')\n",
    "        plt.ylabel('total allocated flow')\n",
    "        ymin, ymax = plt.ylim()\n",
    "        plt.ylim([0,ymax*1.1])\n",
    "        ax.set_xscale('log')\n",
    "        \"\"\"\n",
    "        for p_spec in probs:\n",
    "            break\n",
    "            problem = Problem.from_file(p_spec[1], p_spec[2])\n",
    "            com_list = problem.commodity_list\n",
    "\n",
    "            fig,ax = plt.subplots()\n",
    "\n",
    "            # plot the demand distribution\n",
    "            demands = [ com_list[i][1][2] for i in range(len(com_list)) ]\n",
    "            num_bins = 100\n",
    "            counts, bin_edges = np.histogram(demands, bins=num_bins)\n",
    "            cdf = np.cumsum(counts)\n",
    "            ax.plot(bin_edges[1:], cdf/cdf[-1], c='m', label=\"demands\")\n",
    "\n",
    "            for n_i, n in enumerate(num_subproblems):\n",
    "                merged_sol_dict = defaultdict(int)\n",
    "                for j in range(n):\n",
    "                    sol_dict = sol_dicts[p_spec][n_i][j]\n",
    "\n",
    "\n",
    "                    total_flow = 0\n",
    "                    for commod_key, flow_list in sol_dict.items():\n",
    "                        src, target = commod_key[-1][0], commod_key[-1][1]\n",
    "\n",
    "                        flow = compute_in_or_out_flow(flow_list, 0, {commod_key[-1][0]})\n",
    "\n",
    "                        merged_sol_dict[(src,target)] += flow\n",
    "\n",
    "                    # get amount of flow assigned to each commodity (ASSUMING SINGLE PATH)\n",
    "                    #flow_counts += [ sol_dict[sdf][0][1] for sdf in sol_dict if len(sol_dict[sdf]) > 0 ]\n",
    "\n",
    "                frac_demands_satisfied = {commod_key:\n",
    "                                          merged_sol_dict[(commod_key[-1][0],\n",
    "                                                           commod_key[-1][1])] / commod_key[-1][-1]\n",
    "                                          for commod_key in com_list}\n",
    "\n",
    "                # take frac_demands_satisfied, extract list, plot cdf\n",
    "                num_bins = 100\n",
    "                #print(\"sol_dict: \" + str(sum(flow_counts)) + \", actual obj: \" + str(sum(results[p][n_i])))\n",
    "                counts, bin_edges = np.histogram(flow_counts, bins=num_bins)\n",
    "                cdf = np.cumsum(counts)\n",
    "                ax.plot(bin_edges[1:], cdf/cdf[-1], c=colors[n_i], label=str(n)+\" subproblems\")\n",
    "\n",
    "            plt.xlabel('per-commodity allocated flow')\n",
    "            plt.ylabel('cumulative')\n",
    "            plt.legend()\n",
    "\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#problems = [p6]\n",
    "#plot_benchmark(results_all_obj, runtimes_all_obj, sol_dicts_all_obj, \n",
    "#               results_ncflow, runtimes_ncflow)\n",
    "#results_ncflow = None\n",
    "#runtimes_ncflow = None\n",
    "plot_benchmark(results_all_obj_smart, runtimes_all_obj_smart, \n",
    "               sol_dicts_all_obj_smart, results_ncflow, runtimes_ncflow,\n",
    "               results_cspf, runtimes_cspf)\n",
    "\n",
    "plot_benchmark(results_all_obj_smart_gen, runtimes_all_obj_smart_gen, \n",
    "               sol_dicts_all_obj_smart_gen, results_ncflow, runtimes_ncflow,\n",
    "               results_cspf, runtimes_cspf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_paths, edge_disjoint, dist_metric = PATH_FORM_HYPERPARAMS\n",
    "#problem = Problem.from_file(\"../topologies/topology-zoo/GtsCe.graphml\", \n",
    "#                            \"../traffic-matrices/uniform/GtsCe.graphml_uniform_1475504323_64.0_0.05_traffic-matrix.pkl\")\n",
    "#problem.G\n",
    "#print(problem.G.edges.data())\n",
    "#com_list = problem.commodity_list\n",
    "#problem2 = problem.copy()\n",
    "#print(dir(problem))\n",
    "\n",
    "#pf = PathFormulation.new_max_flow(\n",
    "#                    num_paths,\n",
    "#                    edge_disjoint=edge_disjoint,\n",
    "#                    dist_metric=dist_metric)\n",
    "        \n",
    "#paths_dict = pf.get_paths(problem)\n",
    "\n",
    "#com0 = com_list[3]\n",
    "#print(com0)\n",
    "#print(paths_dict[(com0[1][0],com0[1][1])])\n",
    "\n",
    "\"\"\"\n",
    "problem2.traffic_matrix.tm\n",
    "new_tm = problem2.traffic_matrix.tm[0:10,:]\n",
    "\n",
    "num_rows = len(problem2.traffic_matrix.tm)\n",
    "\n",
    "shuffled_indices = list(range(num_rows))\n",
    "random.shuffle(shuffled_indices)\n",
    "\n",
    "num_first_problem = math.floor(num_rows/2)\n",
    "\n",
    "for i in shuffled_indices[1:num_first_problem]:\n",
    "    problem2.traffic_matrix.tm[i,:] = 0\n",
    "\n",
    "#print(problem2.traffic_matrix.tm[1:5,:])\n",
    "\n",
    "for u,v in problem.G.edges:\n",
    "    problem.G[u][v]['capacity'] = problem.G[u][v]['capacity']/2\n",
    "    problem2.G[u][v]['capacity'] = problem2.G[u][v]['capacity']/2\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CSPF(problems):\n",
    "    \n",
    "    results_all = []\n",
    "    runtimes_all = []\n",
    "    \n",
    "    for problem_name, topo_fname, tm_fname in problems:\n",
    "        problem = Problem.from_file(topo_fname, tm_fname)\n",
    "        \n",
    "        com_list = problem.commodity_list\n",
    "        tm = problem.traffic_matrix.tm\n",
    "        \n",
    "        pf = PathFormulation.new_max_flow(\n",
    "                    num_paths,\n",
    "                    edge_disjoint=edge_disjoint,\n",
    "                    dist_metric=dist_metric)\n",
    "        \n",
    "        paths_dict = pf.get_paths(problem)\n",
    "        \n",
    "        # initialize link capacity dict\n",
    "        remaining_link_capacity_dict = {}\n",
    "        for u,v in problem.G.edges:\n",
    "            remaining_link_capacity_dict[(u,v)] = problem.G[u][v]['capacity']\n",
    "            \n",
    "        # sort paths in ascending order\n",
    "        all_paths_list = []\n",
    "        allocated_coms = {}\n",
    "        for k, (source, target, demand) in com_list:\n",
    "            paths_array = paths_dict[(source, target)]\n",
    "            all_paths_list += paths_array\n",
    "            allocated_coms[(source,target)] = False\n",
    "        all_paths_list.sort(key=len)\n",
    "        \n",
    "        # iterate through sorted paths\n",
    "        total_allocated_flow = 0\n",
    "        startTime = datetime.now()\n",
    "        for path in all_paths_list:\n",
    "            source = path[0]\n",
    "            target = path[-1]\n",
    "            demand = tm[source,target]\n",
    "            \n",
    "            # skip if we have already allocated this commodity\n",
    "            if allocated_coms[(source,target)]:\n",
    "                continue\n",
    "            \n",
    "            # check that each edge in list has enough capacity\n",
    "            edge_list = list(path_to_edge_list(path))\n",
    "            room = True\n",
    "            for u,v in edge_list:\n",
    "                if remaining_link_capacity_dict[(u,v)] < demand:\n",
    "                    room = False\n",
    "                    break\n",
    "            \n",
    "            if not room:\n",
    "                continue\n",
    "            \n",
    "            # allocate\n",
    "            for u,v in edge_list:\n",
    "                remaining_link_capacity_dict[(u,v)] -= demand\n",
    "            allocated_coms[(source, target)] = True\n",
    "            total_allocated_flow += demand\n",
    "        runtime = datetime.now() - startTime\n",
    "        \n",
    "        results_all.append(total_allocated_flow)\n",
    "        runtimes_all.append(runtime.total_seconds())\n",
    "        print(\"Runtime: \" + str(runtime))\n",
    "        print(\"Allocated Flow:\" + str(total_allocated_flow))\n",
    "    return results_all, runtimes_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_cspf, runtimes_cspf = CSPF(problems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_ncflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
