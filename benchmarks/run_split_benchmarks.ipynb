{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark_consts import get_problems, get_args_and_problems, print_, PATH_FORM_HYPERPARAMS\n",
    "\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import traceback\n",
    "import argparse\n",
    "import random\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "from kmodes.kprototypes import KPrototypes\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from lib.algorithms import PathFormulation\n",
    "from lib.problem import Problem\n",
    "from lib.algorithms.abstract_formulation import Objective\n",
    "from lib.graph_utils import compute_in_or_out_flow, path_to_edge_list, assert_flow_conservation, check_feasibility\n",
    "from collections import defaultdict\n",
    "import ncflow\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from datetime import datetime\n",
    "from partition_entities import split_generic, check_dims\n",
    "\n",
    "TOP_DIR = 'path-form-logs'\n",
    "HEADERS = [\n",
    "    'problem', 'num_nodes', 'num_edges', 'traffic_seed', 'scale_factor',\n",
    "    'tm_model', 'num_commodities', 'total_demand', 'algo', 'num_paths',\n",
    "    'edge_disjoint', 'dist_metric', 'total_flow', 'runtime'\n",
    "]\n",
    "PLACEHOLDER = ','.join('{}' for _ in HEADERS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_edges_onehot_dict(problem, pf_original):\n",
    "    paths_dict = pf_original.get_paths(problem)\n",
    "    com_list = problem.commodity_list\n",
    "    num_entities = len(com_list)\n",
    "    num_edges = len(problem.G.edges)\n",
    "    enum_edges_dict = {}\n",
    "    for i, edge in enumerate(problem.G.edges):\n",
    "        enum_edges_dict[edge] = i\n",
    "\n",
    "    # create dictionary of all edges used by each commodity\n",
    "    com_path_edges_dict = defaultdict(list)\n",
    "    min_demand = np.inf\n",
    "    max_demand = 0\n",
    "    for k, (source, target, demand) in com_list:\n",
    "        paths_array = paths_dict[(source, target)]\n",
    "        if min_demand > demand:\n",
    "            min_demand = demand\n",
    "        if max_demand < demand:\n",
    "            max_demand = demand\n",
    "            \n",
    "        for path in paths_array:\n",
    "            com_path_edges_dict[(k, source, target, demand)] += list(path_to_edge_list(path))\n",
    "\n",
    "    com_path_edges_onehot_dict = defaultdict(list)\n",
    "    np_data = np.zeros((num_entities,num_edges+1))\n",
    "    for (k, source, target, demand), edge_list in com_path_edges_dict.items():\n",
    "        onehot_edge = [0]*num_edges\n",
    "        for edge in edge_list:\n",
    "            edge_i = enum_edges_dict[edge]\n",
    "            onehot_edge[edge_i] = 1\n",
    "            np_data[k,edge_i] = 1\n",
    "        # add in normalized demand as a dimension\n",
    "        norm_demand = (demand - min_demand)/(max_demand-min_demand)\n",
    "        com_path_edges_onehot_dict[(k, source, target, demand)] = onehot_edge + [norm_demand]\n",
    "        np_data[k,-1] = norm_demand\n",
    "\n",
    "    return com_path_edges_onehot_dict\n",
    "\n",
    "def split_generic_wrapper(problem, pf, num_subproblems, verbose=False, method='means'):\n",
    "    \n",
    "    input_dict = create_edges_onehot_dict(problem, pf)\n",
    "    \n",
    "    # create subproblems, zero out commodities in traffic matrix that aren't assigned to each \n",
    "    sub_problems = [problem.copy() for _ in range(num_subproblems)]\n",
    "    if num_subproblems == 1:\n",
    "        return sub_problems\n",
    "    \n",
    "    entity_assignments_lists = split_generic(input_dict, num_subproblems, verbose=verbose, method=method)\n",
    "    \n",
    "    for i in range(num_subproblems):\n",
    "\n",
    "        #zero out all commodities not assigned to subproblem i\n",
    "        for ind, source, target, demand in entity_assignments_lists[i]:\n",
    "            for j in range(num_subproblems):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                sub_problems[j].traffic_matrix.tm[source,target] = 0\n",
    "\n",
    "        # split the capacity of each link\n",
    "        for u,v in sub_problems[i].G.edges:\n",
    "            sub_problems[i].G[u][v]['capacity'] = sub_problems[i].G[u][v]['capacity']/num_subproblems\n",
    "\n",
    "    return sub_problems\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dist = [2,5]\n",
    "A = [[1,2,3], [4,5,6]]\n",
    "B = [2,5]\n",
    "C = [3,6]\n",
    "#print(calc_dist(A,B,original_dist))\n",
    "#print(calc_dist(A,C,original_dist))\n",
    "\n",
    "input_dict = {'A': [1,4,3],\n",
    "              'B': [4,3,6],\n",
    "              'C': [7,8,2],\n",
    "              'D': [1,2,3],\n",
    "              'E': [4,5,6],\n",
    "              'F': [9,8,1],\n",
    "              'G': [1,2,4],\n",
    "              'H': [4,2,3],\n",
    "              'I': [7,8,5]}\n",
    "subproblem_list = split_generic(input_dict,2, verbose=True, method='means')\n",
    "check_dims(subproblem_list, input_dict)\n",
    "\n",
    "\n",
    "topo_fname = \"../topologies/topology-zoo/Ion.graphml\"#GtsCe.graphml\"\n",
    "#tm_fname = \"../traffic-matrices/uniform/GtsCe.graphml_uniform_1475504323_64.0_0.05_traffic-matrix.pkl\"\n",
    "tm_fname = \"../traffic-matrices/uniform/Ion.graphml_uniform_1545787193_64.0_0.15_traffic-matrix.pkl\"\n",
    "num_paths, edge_disjoint, dist_metric = PATH_FORM_HYPERPARAMS\n",
    "    \n",
    "pf_original = PathFormulation.new_max_flow(\n",
    "    num_paths,\n",
    "    edge_disjoint=edge_disjoint,\n",
    "    dist_metric=dist_metric)\n",
    "with open('path-form.csv', 'a') as results:\n",
    "    print_(','.join(HEADERS), file=results)\n",
    "    \n",
    "    problem = Problem.from_file(topo_fname, tm_fname)\n",
    "\n",
    "    paths_dict = pf_original.get_paths(problem)\n",
    "    com_list = problem.commodity_list\n",
    "    num_entities = len(com_list)\n",
    "    num_edges = len(problem.G.edges)\n",
    "    enum_edges_dict = {}\n",
    "    for i, edge in enumerate(problem.G.edges):\n",
    "        enum_edges_dict[edge] = i\n",
    "\n",
    "    # create dictionary of all edges used by each commodity\n",
    "    com_path_edges_dict = defaultdict(list)\n",
    "    min_demand = np.inf\n",
    "    max_demand = 0\n",
    "    for k, (source, target, demand) in com_list:\n",
    "        paths_array = paths_dict[(source, target)]\n",
    "        if min_demand > demand:\n",
    "            min_demand = demand\n",
    "        if max_demand < demand:\n",
    "            max_demand = demand\n",
    "            \n",
    "        for path in paths_array:\n",
    "            com_path_edges_dict[(k, source, target, demand)] += list(path_to_edge_list(path))\n",
    "\n",
    "    com_path_edges_onehot_dict = defaultdict(list)\n",
    "    np_data = np.zeros((num_entities,num_edges+1))\n",
    "    for (k, source, target, demand), edge_list in com_path_edges_dict.items():\n",
    "        onehot_edge = [0]*num_edges\n",
    "        for edge in edge_list:\n",
    "            edge_i = enum_edges_dict[edge]\n",
    "            onehot_edge[edge_i] = 1\n",
    "            np_data[k,edge_i] = 1\n",
    "        # add in normalized demand as a dimension\n",
    "        norm_demand = (demand - min_demand)/(max_demand-min_demand)\n",
    "        com_path_edges_onehot_dict[k] = onehot_edge + [norm_demand]\n",
    "        np_data[k,-1] = norm_demand\n",
    "    \n",
    "    #print(\"creating subproblem list...\")\n",
    "    #subproblem_list_meansplit = split_generic(com_path_edges_onehot_dict, 2, verbose=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp = KPrototypes(n_clusters=20, init='Huang', n_init=1, verbose=1, n_jobs=24, max_iter=8)\n",
    "start_time = time.time()\n",
    "clusters = kp.fit_predict(np_data, categorical=[ x for x in range(0,num_edges)])\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths_dict: key: (source, target), value: array of paths,\n",
    "#             where a path is a list of sequential nodes\n",
    "#             use lib.graph_utils.path_to_edge_list to get edges.\n",
    "def split_problem_smartpath(problem, num_subproblems, paths_dict):\n",
    "    com_list = problem.commodity_list\n",
    "\n",
    "    # create dictionary of all edges used by each commodity\n",
    "    com_path_edges_dict = defaultdict(list)\n",
    "    for k, (source, target, demand) in com_list:\n",
    "        paths_array = paths_dict[(source, target)]\n",
    "        for path in paths_array:\n",
    "            com_path_edges_dict[(k, source, target)] += list(path_to_edge_list(path))\n",
    "\n",
    "    # for each edge, split all commodities using that edge across subproblems\n",
    "    subproblem_com_indices = defaultdict(list)\n",
    "    current_subproblem = 0\n",
    "    for (u,v) in problem.G.edges:\n",
    "        coms_on_edge = [x for x in com_path_edges_dict.keys() if (u,v) in com_path_edges_dict[x]]\n",
    "\n",
    "        # split commodities that share path across all subproblems\n",
    "        for (k, source, target) in coms_on_edge:\n",
    "            subproblem_com_indices[current_subproblem] += [(k,source,target)]\n",
    "            current_subproblem = (current_subproblem + 1) % num_subproblems\n",
    "            # remove commodity from cosideration when processing later edges\n",
    "            del com_path_edges_dict[(k, source, target)]\n",
    "\n",
    "    # create subproblems, zero out commodities in traffic matrix that aren't assigned to each \n",
    "    sub_problems = []\n",
    "    for i in range(num_subproblems):\n",
    "\n",
    "        sub_problems.append(problem.copy())\n",
    "\n",
    "        #zero out all commodities not assigned to subproblem i\n",
    "        for k in subproblem_com_indices.keys():\n",
    "            if k == i:\n",
    "                continue\n",
    "            zero_out_list = subproblem_com_indices[k]\n",
    "            for ind, source, target in zero_out_list:\n",
    "                sub_problems[-1].traffic_matrix.tm[source,target] = 0\n",
    "\n",
    "        # split the capacity of each link\n",
    "        for u,v in sub_problems[-1].G.edges:\n",
    "            sub_problems[-1].G[u][v]['capacity'] = sub_problems[-1].G[u][v]['capacity']/num_subproblems\n",
    "\n",
    "    return sub_problems\n",
    "\n",
    "#Input: a Problem, and a list of number representing the divisions\n",
    "def split_problem(problem, num_subproblems):\n",
    "    sub_problems = []\n",
    "    num_rows = len(problem.traffic_matrix.tm)\n",
    "    rows_per_problem = math.floor(num_rows/num_subproblems)\n",
    "    shuffled_indices = list(range(num_rows))\n",
    "\n",
    "    for i in range(num_subproblems):\n",
    "\n",
    "        sub_problems.append(problem.copy())\n",
    "        for indx, j in enumerate(shuffled_indices):\n",
    "\n",
    "            # zero out all rows except those in the corresponding block of shuffled indices\n",
    "            # first, cover special case for last block\n",
    "            if i == num_subproblems-1:\n",
    "                if indx < i*rows_per_problem:\n",
    "                    sub_problems[-1].traffic_matrix.tm[j,:] = 0\n",
    "\n",
    "            elif (indx < i*rows_per_problem) or (indx >= (i+1)*rows_per_problem):\n",
    "                sub_problems[-1].traffic_matrix.tm[j,:] = 0\n",
    "\n",
    "        # split the capacity of each link\n",
    "        for u,v in sub_problems[-1].G.edges:\n",
    "            sub_problems[-1].G[u][v]['capacity'] = sub_problems[-1].G[u][v]['capacity']/num_subproblems\n",
    "\n",
    "    return sub_problems\n",
    "\n",
    "# assign commodities to subproblems at random\n",
    "def split_random(problem, num_subproblems):\n",
    "    sub_problems = [problem.copy() for _ in range(num_subproblems)]\n",
    "    num_coms = len(problem.commodity_list)\n",
    "    \n",
    "    for k, (source, target, demand) in problem.commodity_list:\n",
    "        sp_assignment = random.randint(0,num_subproblems-1)\n",
    "        for sp in range(num_subproblems):\n",
    "            if sp == sp_assignment:\n",
    "                continue\n",
    "            sub_problems[sp].traffic_matrix.tm[source,target] = 0\n",
    "    for sub_problem in sub_problems:\n",
    "        for u,v in sub_problems[-1].G.edges:\n",
    "            sub_problem.G[u][v]['capacity'] = sub_problem.G[u][v]['capacity']/num_subproblems\n",
    "    return sub_problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sweep topos and traffic matrices for that topo. For each combo, record the\n",
    "# runtime and total flow for each algorithm\n",
    "# split can be random, means, covs, clusters, tailored, or skewed\n",
    "def benchmark_split(problems, num_subproblems_list, obj, split='random'):\n",
    "    num_paths, edge_disjoint, dist_metric = PATH_FORM_HYPERPARAMS\n",
    "\n",
    "    if (obj == 'max_flow'):\n",
    "        pf_original = PathFormulation.new_max_flow(\n",
    "            num_paths,\n",
    "            edge_disjoint=edge_disjoint,\n",
    "            dist_metric=dist_metric)\n",
    "    elif (obj == 'min_max_link_util'):\n",
    "        pf_original = PathFormulation.new_min_max_link_util(\n",
    "            num_paths,\n",
    "            edge_disjoint=edge_disjoint,\n",
    "            dist_metric=dist_metric)\n",
    "    else:\n",
    "        print(obj + \" not supported\")\n",
    "        return\n",
    "\n",
    "    all_results = {}\n",
    "    all_runtimes = {}\n",
    "    all_sol_dicts = {}\n",
    "    with open('path-form.csv', 'a') as results:\n",
    "        print_(','.join(HEADERS), file=results)\n",
    "        for problem_name, topo_fname, tm_fname in problems:\n",
    "            problem = Problem.from_file(topo_fname, tm_fname)\n",
    "\n",
    "            paths_dict = pf_original.get_paths(problem)\n",
    "\n",
    "            problem_results = [[] for _ in range(len(num_subproblems_list))]\n",
    "            problem_runtimes = [[] for _ in range(len(num_subproblems_list))]\n",
    "            problem_sol_dicts = [[] for _ in range(len(num_subproblems_list))]\n",
    "            for nsp_i, num_subproblems in enumerate(num_subproblems_list):\n",
    "                \n",
    "                if split == 'tailored':\n",
    "                    problem_list = split_problem_smartpath(problem, num_subproblems, paths_dict)\n",
    "                elif split == 'skewed':\n",
    "                    problem_list = split_problem(problem, num_subproblems)\n",
    "                elif split == 'random':\n",
    "                    problem_list = split_random(problem, num_subproblems)\n",
    "                elif split == 'means' or split == 'covs':\n",
    "                    problem_list = split_generic_wrapper(problem, pf_original, \n",
    "                                                         num_subproblems, method=split)\n",
    "              \n",
    "                sum_obj_val = 0\n",
    "                for sp_i, sub_problem in enumerate(problem_list):\n",
    "\n",
    "                    print_(sub_problem.name, tm_fname)\n",
    "                    traffic_seed = sub_problem.traffic_matrix.seed\n",
    "                    total_demand = sub_problem.total_demand\n",
    "                    print_('traffic seed: {}'.format(traffic_seed))\n",
    "                    print_('traffic scale factor: {}'.format(\n",
    "                        sub_problem.traffic_matrix.scale_factor))\n",
    "                    print_('traffic matrix model: {}'.format(\n",
    "                        sub_problem.traffic_matrix.model))\n",
    "                    print_('total demand: {}'.format(total_demand))\n",
    "\n",
    "                    run_dir = os.path.join(\n",
    "                        TOP_DIR, sub_problem.name,\n",
    "                        '{}-{}'.format(traffic_seed, sub_problem.traffic_matrix.model))\n",
    "                    if not os.path.exists(run_dir):\n",
    "                        os.makedirs(run_dir)\n",
    "\n",
    "                    try:\n",
    "                        print_(\n",
    "                            '\\\\nPath formulation, {} paths, edge disjoint {}, dist metric {}'\n",
    "                            .format(num_paths, edge_disjoint, dist_metric))\n",
    "                        with open(\n",
    "                                os.path.join(\n",
    "                                    run_dir,\n",
    "                                    '{}-path-formulation_{}-paths_edge-disjoint-{}_dist-metric-{}.txt'\n",
    "                                    .format(sub_problem.name, num_paths, edge_disjoint,\n",
    "                                            dist_metric)), 'w') as log:\n",
    "\n",
    "                            if (obj == 'max_flow'):\n",
    "                                pf = PathFormulation.new_max_flow(\n",
    "                                    num_paths,\n",
    "                                    edge_disjoint=edge_disjoint,\n",
    "                                    dist_metric=dist_metric,\n",
    "                                    out=log)\n",
    "                            elif (obj == 'min_max_link_util'):\n",
    "                                pf = PathFormulation.new_min_max_link_util(\n",
    "                                    num_paths,\n",
    "                                    edge_disjoint=edge_disjoint,\n",
    "                                    dist_metric=dist_metric,\n",
    "                                    out=log)\n",
    "                            else:\n",
    "                                print(obj + \" not supported\")\n",
    "                                break\n",
    "\n",
    "                            pf.solve(sub_problem)\n",
    "                            pf_sol_dict = pf.extract_sol_as_dict()\n",
    "                            with open(\n",
    "                                    os.path.join(\n",
    "                                        run_dir,\n",
    "                                        '{}-path-formulation_{}-paths_edge-disjoint-{}_dist-metric-{}_sol-dict.pkl'\n",
    "                                        .format(sub_problem.name, num_paths, edge_disjoint,\n",
    "                                                dist_metric)), 'wb') as w:\n",
    "                                pickle.dump(pf_sol_dict, w)\n",
    "\n",
    "                        result_line = PLACEHOLDER.format(\n",
    "                            sub_problem.name,\n",
    "                            len(sub_problem.G.nodes),\n",
    "                            len(sub_problem.G.edges),\n",
    "                            traffic_seed,\n",
    "                            sub_problem.traffic_matrix.scale_factor,\n",
    "                            sub_problem.traffic_matrix.model,\n",
    "                            len(problem.commodity_list),\n",
    "                            total_demand,\n",
    "                            'path_formulation',\n",
    "                            num_paths,\n",
    "                            edge_disjoint,\n",
    "                            dist_metric,\n",
    "                            pf.obj_val,\n",
    "                            pf.runtime,\n",
    "                        )\n",
    "                        print_(result_line, file=results)\n",
    "                        problem_results[nsp_i].append(pf.obj_val)\n",
    "                        problem_runtimes[nsp_i].append(pf.runtime)\n",
    "                        problem_sol_dicts[nsp_i].append(pf_sol_dict)\n",
    "                        sum_obj_val += pf.obj_val\n",
    "                    except Exception:\n",
    "                        print_(\n",
    "                            'Path formulation {} paths, edge disjoint {}, dist metric {}, Problem {}, traffic seed {}, traffic model {} failed'\n",
    "                            .format(num_paths, edge_disjoint, dist_metric,\n",
    "                                    sub_problem.name, traffic_seed,\n",
    "                                    sub_problem.traffic_matrix.model))\n",
    "                        traceback.print_exc(file=sys.stdout)\n",
    "                print(\"sum of obj vals: \" + str(sum_obj_val))\n",
    "\n",
    "            all_results[(problem_name, topo_fname, tm_fname)] = problem_results\n",
    "            all_runtimes[(problem_name, topo_fname, tm_fname)] = problem_runtimes\n",
    "            all_sol_dicts[(problem_name, topo_fname, tm_fname)] = problem_sol_dicts\n",
    "    return all_results, all_runtimes, all_sol_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dry-run',\n",
    "                        dest='dry_run',\n",
    "                        action='store_true',\n",
    "                        default=False)\n",
    "\n",
    "parser.add_argument('--slices',\n",
    "                        type=int,\n",
    "                        choices=range(5),\n",
    "                        nargs='+',\n",
    "                        required=True)\n",
    "args = parser.parse_args(\"--slices 0\".split())\n",
    "\n",
    "if not os.path.exists(TOP_DIR):\n",
    "    os.makedirs(TOP_DIR)\n",
    "\n",
    "#problems = get_problems(args)\n",
    "\n",
    "if args.dry_run:\n",
    "    print('Problems to run:')\n",
    "    for problem in problems:\n",
    "        print(problem)\n",
    "\n",
    "p1 = (\"uniform 64\", \"../topologies/topology-zoo/GtsCe.graphml\", \n",
    "                        \"../traffic-matrices/uniform/GtsCe.graphml_uniform_1475504323_64.0_0.05_traffic-matrix.pkl\")\n",
    "p2 = (\"poisson-inter 128\", \"../topologies/topology-zoo/GtsCe.graphml\", \n",
    "                        \"../traffic-matrices/poisson-high-inter/GtsCe.graphml_poisson_2035531455_128.0_1000.0_0.9_8.5e-05_traffic-matrix.pkl\")\n",
    "p3 = (\"poisson-intra 128\", \"../topologies/topology-zoo/GtsCe.graphml\", \n",
    "                        \"../traffic-matrices/poisson-high-intra/GtsCe.graphml_poisson_1367969278_128.0_200000000.0_0.1_2.25e-06_traffic-matrix.pkl\")\n",
    "p4 = (\"uniform 8\", \"../topologies/topology-zoo/GtsCe.graphml\", \n",
    "                        \"../traffic-matrices/uniform/GtsCe.graphml_uniform_19019979_8.0_0.05_traffic-matrix.pkl\")\n",
    "\n",
    "p5 = (\"kdl_16\", \"../topologies/topology-zoo/Kdl.graphml\",\n",
    "         \"../traffic-matrices/gravity/Kdl.graphml_gravity_1710674203_16.0_24000.064453125_True_traffic-matrix.pkl\")\n",
    "p6 = (\"kdl_32\", \"../topologies/topology-zoo/Kdl.graphml\",\n",
    "         \"../traffic-matrices/gravity/Kdl.graphml_gravity_1836337794_32.0_48001.9765625_True_traffic-matrix.pkl\")\n",
    "p7 = (\"kdl_64\", \"../topologies/topology-zoo/Kdl.graphml\",\n",
    "         \"../traffic-matrices/gravity/Kdl.graphml_gravity_530572184_64.0_95909.890625_True_traffic-matrix.pkl\")\n",
    "p8 = (\"kdl_128\", \"../topologies/topology-zoo/Kdl.graphml\",\n",
    "         \"../traffic-matrices/gravity/Kdl.graphml_gravity_378818577_128.0_192067.765625_True_traffic-matrix.pkl\")\n",
    "\n",
    "p9 = (\"cogentco_16\", \"../topologies/topology-zoo/Cogentco.graphml\",\n",
    "         \"../traffic-matrices/gravity/Cogentco.graphml_gravity_1443251160_16.0_19189.4609375_True_traffic-matrix.pkl\")\n",
    "p10 = (\"cogentco_32\", \"../topologies/topology-zoo/Cogentco.graphml\",\n",
    "         \"../traffic-matrices/gravity/Cogentco.graphml_gravity_1625678606_32.0_38304.9609375_True_traffic-matrix.pkl\")\n",
    "p11 = (\"cogentco_64\", \"../topologies/topology-zoo/Cogentco.graphml\",\n",
    "         \"../traffic-matrices/gravity/Cogentco.graphml_gravity_1693196370_64.0_76793.84375_True_traffic-matrix.pkl\")\n",
    "p12 = (\"cogentco_128\", \"../topologies/topology-zoo/Cogentco.graphml\",\n",
    "         \"../traffic-matrices/gravity/Cogentco.graphml_gravity_1882736072_128.0_153591.515625_True_traffic-matrix.pkl\")\n",
    "\n",
    "p13 = (\"kdl_8\", \"../topologies/topology-zoo/Kdl.graphml\",\n",
    "         \"../traffic-matrices/gravity/Kdl.graphml_gravity_2139423624_8.0_11990.4140625_True_traffic-matrix.pkl\")\n",
    "\n",
    "p14 = (\"ER_32\", \"../topologies/erdos-renyi-1260231677.json\",\n",
    "       \"../traffic-matrices/gravity/erdos-renyi-1260231677.json_gravity_479716829_32.0_15993702.0_True_traffic-matrix.pkl\")\n",
    "\n",
    "p15 = (\"ion_64\", \"../topologies/topology-zoo/Ion.graphml\",\n",
    "       \"../traffic-matrices/uniform/Ion.graphml_uniform_1545787193_64.0_0.15_traffic-matrix.pkl\")\n",
    "\n",
    "#num_subproblems = [64]\n",
    "num_subproblems = [1,2]#,4,8,16,32]\n",
    "#num_subproblems = [1,8]\n",
    "#problems = [(os.path.basename(p1[1]), p1[1], p1[2])]\n",
    "problems = [p15]#,p10,p11]\n",
    "#problems = [p5,p6,p7,p8,p9,p10,p11,p12]\n",
    "\n",
    "obj_types = [\"max_flow\"]#, \"min_max_link_util\"]\n",
    "#obj_types = [\"min_max_link_util\"]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "obj_type = 'max_flow'\n",
    "split_methods = ['random', 'skewed', 'tailored']#, 'covs', 'skewed', 'tailored']\n",
    "results_all = {}\n",
    "runtimes_all = {}\n",
    "sol_dicts_all = {}\n",
    "for method in split_methods:\n",
    "    \n",
    "    results, runtimes, sol_dicts = benchmark_split(problems, num_subproblems, obj_type, split=method)\n",
    "    results_all[method] = results\n",
    "    runtimes_all[method] = runtimes\n",
    "    sol_dicts_all[method] = sol_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['k', 'r', 'b', 'g', 'm']\n",
    "linestyles = ['-', '--', ':']\n",
    "#print(results_ncflow)\n",
    "#results_ncflow = results_ncflow[1::2]\n",
    "#print(runtimes_ncflow)\n",
    "def compute_obj_val(obj, problem, obj_vals, sol_dicts):\n",
    "    if obj == 'max_flow':\n",
    "        return sum(obj_vals)\n",
    "    elif obj == 'min_max_link_util':\n",
    "        link_utils_dict = defaultdict(int)\n",
    "        for sol_dict in sol_dicts:\n",
    "            for flow_list in sol_dict.values():\n",
    "                for ((u, v), flow_val) in flow_list: # TODO: is this right? Or is it ((u, v), flow_val)?\n",
    "                    link_utils_dict[(u, v)] += flow_val\n",
    "        \n",
    "        # TODO: is this right? Or is it ((u, v), c_e)?\n",
    "        link_utils = {(u,v): link_utils_dict[(u, v)]/c_e for (u,v,c_e) in problem.G.edges.data('capacity')}\n",
    "        return max(link_utils.values())\n",
    "    \n",
    "# run the benchmarks, plot results\n",
    "def plot_split_results(results_all, runtimes_all, sol_dicts_all, \n",
    "                  results_cspf, runtimes_cspf):\n",
    "    fig_results, ax_results = plt.subplots()\n",
    "    plt.xlabel('number of subproblems')\n",
    "    plt.ylabel('total flow')\n",
    "    \n",
    "    fig_rr, ax_rr = plt.subplots()\n",
    "    \n",
    "    plt.xlabel('execution time (seconds)')\n",
    "    plt.ylabel('total allocated flow')\n",
    "    #ymin, ymax = plt.ylim()\n",
    "    #plt.ylim([0,ymax*1.1])\n",
    "    \n",
    "    for m_i, method in enumerate(split_methods):\n",
    "        \n",
    "        results = results_all[method]\n",
    "        runtimes = runtimes_all[method]\n",
    "        sol_dicts = sol_dicts_all[method]\n",
    "      \n",
    "        \n",
    "        for p_i, p in enumerate(problems):\n",
    "            total_obj_values = []\n",
    "            for n_i, n in enumerate(num_subproblems):\n",
    "                total_obj_val = compute_obj_val(obj_type, Problem.from_file(p[1], p[2]),\n",
    "                                                results[p][n_i], sol_dicts[p][n_i])\n",
    "        #         sum_val = sum(results[p][n_i])\n",
    "                total_obj_values.append(total_obj_val)\n",
    "            ax_results.plot(num_subproblems, total_obj_values, marker='*', c=colors[m_i%len(colors)], \n",
    "                            linestyle=linestyles[m_i%len(linestyles)], label=method)\n",
    "\n",
    "        # plot performance vs runtime.\n",
    "        for p_i, p in enumerate(problems):\n",
    "            total_obj_values = []\n",
    "            total_runtime = []\n",
    "            max_runtime = []\n",
    "            for n_i, n in enumerate(num_subproblems):\n",
    "                total_obj_val = compute_obj_val(obj_type, Problem.from_file(p[1], p[2]),\n",
    "                                                results[p][n_i], sol_dicts[p][n_i])\n",
    "                total_obj_values.append(total_obj_val)\n",
    "\n",
    "                sum_val = sum(runtimes[p][n_i])\n",
    "                max_time = max(runtimes[p][n_i])\n",
    "                total_runtime.append(sum_val)\n",
    "                max_runtime.append(max_time)\n",
    "            print(max_runtime)\n",
    "            print(total_obj_values)\n",
    "            ax_rr.scatter(max_runtime, total_obj_values, marker='*', c=colors[m_i%len(colors)], label=method)\n",
    "            for n_i, n in enumerate(num_subproblems):\n",
    "                ax_rr.annotate(n, (max_runtime[n_i], total_obj_values[n_i]))\n",
    "            #if (obj_type == \"max_flow\"):\n",
    "                #also plot runtime of ncflow\n",
    "            #    ax.plot(runtimes_ncflow[p_i], results_ncflow[p_i], marker='P', \n",
    "            #            markersize=10, c=colors[p_i%len(colors)], label=p[0]+\"; ncflow\")\n",
    "            ax_rr.plot(runtimes_cspf[p_i], results_cspf[p_i], marker='x', \n",
    "                    markersize=10, c=colors[p_i%len(colors)], label=p[0]+\"; CSPF\")\n",
    "    ax_results.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax_rr.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax_rr.set_xscale('log')    \n",
    "\n",
    "plot_split_results(results_all, runtimes_all, \n",
    "               sol_dicts_all,\n",
    "               results_cspf, runtimes_cspf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Firas's Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_name, topo_fname, tm_fname = p8\n",
    "prob = Problem.from_file(topo_fname, tm_fname)\n",
    "NUM_PATHS = 4\n",
    "NUM_SUBPROBLEMS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_subproblems(problem_list):\n",
    "    sol_dicts, runtimes, obj_vals = [], [], []\n",
    "    for sub_problem in problem_list:\n",
    "        pf = PathFormulation.get_pf_for_obj(Objective.MAX_FLOW, NUM_PATHS)\n",
    "        pf.solve(sub_problem)\n",
    "        sol_dicts.append(pf.extract_sol_as_dict())\n",
    "        runtimes.append(pf.runtime)\n",
    "        obj_vals.append(pf.obj_val)\n",
    "    #check\n",
    "    return sol_dicts, runtimes, obj_vals\n",
    "\n",
    "def solve_and_check_feasiblity(problem, num_subproblems, num_paths):    \n",
    "    paths_dict = PathFormulation.new_max_flow(num_paths).get_paths(problem)\n",
    "    problem_list = split_problem_smartpath(prob, num_subproblems, paths_dict)\n",
    "    sol_dicts, runtimes, obj_vals = solve_subproblems(problem_list)\n",
    "    check_feasibility(problem, sol_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "solve_and_check_feasiblity(prob, NUM_SUBPROBLEMS, NUM_PATHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End Firas's Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_solution(sol_dicts_all, num_subproblems):\n",
    "    for obj_type in obj_types:\n",
    "        sol_dicts = sol_dicts_all[obj_type]\n",
    "        for p_spec in problems:\n",
    "            problem = Problem.from_file(p_spec[1], p_spec[2a])\n",
    "            com_list = problem.commodity_list\n",
    "            \n",
    "            for n_i, n in enumerate(num_subproblems):\n",
    "                merged_sol_dict = defaultdict(int)\n",
    "                for j in range(n):\n",
    "                    sol_dict = sol_dicts[p_spec][n_i][j]\n",
    "                    total_flow = 0\n",
    "                    for commod_key, flow_list in sol_dict.items():\n",
    "                        assert_flow_conservation(flow_list, commod_key)\n",
    "                        src, target = commod_key[-1][0], commod_key[-1][1]\n",
    "\n",
    "                        flow = compute_in_or_out_flow(flow_list, 0, {commod_key[-1][0]})\n",
    "\n",
    "                        merged_sol_dict[(src,target)] += flow\n",
    "            \n",
    "                frac_demands_satisfied = {commod_key:\n",
    "                                      merged_sol_dict[(commod_key[-1][0],\n",
    "                                                       commod_key[-1][1])] / commod_key[-1][-1]\n",
    "                                      for commod_key in com_list}\n",
    "                for commod_key, frac in frac_demands_satisfied.items():\n",
    "                    if frac > 1:\n",
    "                        print(\"assertion error, demand oversatisfied \"+ str(commod_key) + \" \" + str(frac))\n",
    "                        break\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_all_obj = {}\n",
    "runtimes_all_obj = {}\n",
    "sol_dicts_all_obj = {}\n",
    "for obj_type in obj_types:\n",
    "    \n",
    "    results, runtimes, sol_dicts = benchmark_split(problems, num_subproblems, obj_type, smart=False)\n",
    "    results_all_obj[obj_type] = results\n",
    "    runtimes_all_obj[obj_type] = runtimes\n",
    "    sol_dicts_all_obj[obj_type] = sol_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_all_obj_smart = {}\n",
    "runtimes_all_obj_smart = {}\n",
    "sol_dicts_all_obj_smart = {}\n",
    "for obj_type in obj_types:\n",
    "    \n",
    "    results, runtimes, sol_dicts = benchmark_split(problems, num_subproblems, obj_type, smart=True, generic=False)\n",
    "    results_all_obj_smart[obj_type] = results\n",
    "    runtimes_all_obj_smart[obj_type] = runtimes\n",
    "    sol_dicts_all_obj_smart[obj_type] = sol_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_all_obj_smart_gen = {}\n",
    "runtimes_all_obj_smart_gen = {}\n",
    "sol_dicts_all_obj_smart_gen = {}\n",
    "for obj_type in obj_types:\n",
    "    \n",
    "    results, runtimes, sol_dicts = benchmark_split(problems, num_subproblems, obj_type, smart=True, generic=True)\n",
    "    results_all_obj_smart_gen[obj_type] = results\n",
    "    runtimes_all_obj_smart_gen[obj_type] = runtimes\n",
    "    sol_dicts_all_obj_smart_gen[obj_type] = sol_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smart_results = [results_all_obj_smart, runtimes_all_obj_smart, sol_dicts_all_obj_smart]\n",
    "pickle.dump(smart_results, open(\"results/smart_results.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[results_all_obj_smart, \n",
    "runtimes_all_obj_smart, \n",
    "sol_dicts_all_obj_smart] = pickle.load(open(\"results/smart_results.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run NCFlow on problem set\n",
    "problems_ncflow = [(os.path.basename(p[1]), p[1], p[2]) for p in problems]\n",
    "results_ncflow, runtimes_ncflow = ncflow.benchmark(problems_ncflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['k', 'r', 'b', 'g']\n",
    "linestyles = ['-', '--', ':']\n",
    "#print(results_ncflow)\n",
    "#results_ncflow = results_ncflow[1::2]\n",
    "#print(runtimes_ncflow)\n",
    "def compute_obj_val(obj, problem, obj_vals, sol_dicts):\n",
    "    if obj == 'max_flow':\n",
    "        return sum(obj_vals)\n",
    "    elif obj == 'min_max_link_util':\n",
    "        link_utils_dict = defaultdict(int)\n",
    "        for sol_dict in sol_dicts:\n",
    "            for flow_list in sol_dict.values():\n",
    "                for ((u, v), flow_val) in flow_list: # TODO: is this right? Or is it ((u, v), flow_val)?\n",
    "                    link_utils_dict[(u, v)] += flow_val\n",
    "        \n",
    "        # TODO: is this right? Or is it ((u, v), c_e)?\n",
    "        link_utils = {(u, v): link_utils_dict[(u, v)] / c_e for (u, v, c_e) in problem.G.edges.data('capacity')}\n",
    "        return max(link_utils.values())\n",
    "    \n",
    "# run the benchmarks, plot results\n",
    "def plot_benchmark(results_all, runtimes_all, sol_dicts_all, results_ncflow, runtimes_ncflow,\n",
    "                  results_cspf, runtimes_cspf):\n",
    "    for obj_type in obj_types:\n",
    "        \n",
    "        results = results_all[obj_type]\n",
    "        runtimes = runtimes_all[obj_type]\n",
    "        sol_dicts = sol_dicts_all[obj_type]\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        for p_i, p in enumerate(problems):\n",
    "            total_obj_values = []\n",
    "            for n_i, n in enumerate(num_subproblems):\n",
    "                total_obj_val = compute_obj_val(obj_type, Problem.from_file(p[1], p[2]),\n",
    "                                                results[p][n_i], sol_dicts[p][n_i])\n",
    "        #         sum_val = sum(results[p][n_i])\n",
    "                total_obj_values.append(total_obj_val)\n",
    "            ax.plot(num_subproblems, total_obj_values, marker='*', c=colors[p_i%len(colors)], linestyle=linestyles[p_i%len(linestyles)],\n",
    "                    label=p[0])\n",
    "            #if (obj_type == \"max_flow\"):\n",
    "                #also plot flow achieved by ncflow\n",
    "                #ax.plot(1, results_ncflow[p_i], marker='P', markersize=10, c=colors[p_i%len(colors)], label=p[0]+\"; ncflow\")\n",
    "        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.xlabel('number of subproblems')\n",
    "        plt.ylabel('total flow')\n",
    "        #plt.ylim([0,None])\n",
    "        #plt.savefig('./plots/flow_'+p[0]+'.png', bbox_inches='tight')\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        for p_i, p in enumerate(problems):\n",
    "            total_runtime = []\n",
    "            max_runtime = []\n",
    "            for n_i, n in enumerate(num_subproblems):\n",
    "                sum_val = sum(runtimes[p][n_i])\n",
    "                print(sum_val)\n",
    "                max_time = max(runtimes[p][n_i])\n",
    "                total_runtime.append(sum_val)\n",
    "                max_runtime.append(max_time)\n",
    "            ax.plot(num_subproblems, total_runtime, marker='*', linestyle=linestyles[0], \n",
    "                    c=colors[p_i%len(colors)], label='sum ' + p[0])\n",
    "            ax.plot(num_subproblems, max_runtime, marker='^', linestyle=linestyles[1], \n",
    "                    c=colors[p_i%len(colors)], label='max ' + p[0])\n",
    "            if (obj_type == \"max_flow\"):\n",
    "                #also plot runtime of ncflow\n",
    "                ax.plot(1, runtimes_ncflow[p_i], marker='P', markersize=10, c=colors[p_i%len(colors)], label=p[0]+\"; ncflow\")\n",
    "        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.xlabel('number of subproblems')\n",
    "        plt.ylabel('execution time (seconds)')\n",
    "        plt.ylim([0,None])\n",
    "        #plt.savefig('./plots/runtime_'+p[0]+'.png', bbox_inches='tight')\n",
    "\n",
    "        # plot performance vs runtime.\n",
    "        fig, ax = plt.subplots()\n",
    "        for p_i, p in enumerate(problems):\n",
    "            total_obj_values = []\n",
    "            total_runtime = []\n",
    "            max_runtime = []\n",
    "            for n_i, n in enumerate(num_subproblems):\n",
    "                total_obj_val = compute_obj_val(obj_type, Problem.from_file(p[1], p[2]),\n",
    "                                                results[p][n_i], sol_dicts[p][n_i])\n",
    "                total_obj_values.append(total_obj_val)\n",
    "\n",
    "                sum_val = sum(runtimes[p][n_i])\n",
    "                max_time = max(runtimes[p][n_i])\n",
    "                total_runtime.append(sum_val)\n",
    "                max_runtime.append(max_time)\n",
    "            print(max_runtime)\n",
    "            print(total_obj_values)\n",
    "            ax.scatter(max_runtime, total_obj_values, marker='*', c=colors[p_i%len(colors)], label=p[0])\n",
    "            for n_i, n in enumerate(num_subproblems):\n",
    "                ax.annotate(n, (max_runtime[n_i], total_obj_values[n_i]))\n",
    "            if (obj_type == \"max_flow\"):\n",
    "                #also plot runtime of ncflow\n",
    "                ax.plot(runtimes_ncflow[p_i], results_ncflow[p_i], marker='P', markersize=10, c=colors[p_i%len(colors)], label=p[0]+\"; ncflow\")\n",
    "                ax.plot(runtimes_cspf[p_i], results_cspf[p_i], marker='x', markersize=10, c=colors[p_i%len(colors)], label=p[0]+\"; CSPF\")\n",
    "        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.xlabel('execution time (seconds)')\n",
    "        plt.ylabel('total allocated flow')\n",
    "        ymin, ymax = plt.ylim()\n",
    "        plt.ylim([0,ymax*1.1])\n",
    "        ax.set_xscale('log')\n",
    "        \"\"\"\n",
    "        for p_spec in probs:\n",
    "            break\n",
    "            problem = Problem.from_file(p_spec[1], p_spec[2])\n",
    "            com_list = problem.commodity_list\n",
    "\n",
    "            fig,ax = plt.subplots()\n",
    "\n",
    "            # plot the demand distribution\n",
    "            demands = [ com_list[i][1][2] for i in range(len(com_list)) ]\n",
    "            num_bins = 100\n",
    "            counts, bin_edges = np.histogram(demands, bins=num_bins)\n",
    "            cdf = np.cumsum(counts)\n",
    "            ax.plot(bin_edges[1:], cdf/cdf[-1], c='m', label=\"demands\")\n",
    "\n",
    "            for n_i, n in enumerate(num_subproblems):\n",
    "                merged_sol_dict = defaultdict(int)\n",
    "                for j in range(n):\n",
    "                    sol_dict = sol_dicts[p_spec][n_i][j]\n",
    "\n",
    "\n",
    "                    total_flow = 0\n",
    "                    for commod_key, flow_list in sol_dict.items():\n",
    "                        src, target = commod_key[-1][0], commod_key[-1][1]\n",
    "\n",
    "                        flow = compute_in_or_out_flow(flow_list, 0, {commod_key[-1][0]})\n",
    "\n",
    "                        merged_sol_dict[(src,target)] += flow\n",
    "\n",
    "                    # get amount of flow assigned to each commodity (ASSUMING SINGLE PATH)\n",
    "                    #flow_counts += [ sol_dict[sdf][0][1] for sdf in sol_dict if len(sol_dict[sdf]) > 0 ]\n",
    "\n",
    "                frac_demands_satisfied = {commod_key:\n",
    "                                          merged_sol_dict[(commod_key[-1][0],\n",
    "                                                           commod_key[-1][1])] / commod_key[-1][-1]\n",
    "                                          for commod_key in com_list}\n",
    "\n",
    "                # take frac_demands_satisfied, extract list, plot cdf\n",
    "                num_bins = 100\n",
    "                #print(\"sol_dict: \" + str(sum(flow_counts)) + \", actual obj: \" + str(sum(results[p][n_i])))\n",
    "                counts, bin_edges = np.histogram(flow_counts, bins=num_bins)\n",
    "                cdf = np.cumsum(counts)\n",
    "                ax.plot(bin_edges[1:], cdf/cdf[-1], c=colors[n_i], label=str(n)+\" subproblems\")\n",
    "\n",
    "            plt.xlabel('per-commodity allocated flow')\n",
    "            plt.ylabel('cumulative')\n",
    "            plt.legend()\n",
    "\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#problems = [p6]\n",
    "#plot_benchmark(results_all_obj, runtimes_all_obj, sol_dicts_all_obj, \n",
    "#               results_ncflow, runtimes_ncflow)\n",
    "#results_ncflow = None\n",
    "#runtimes_ncflow = None\n",
    "plot_benchmark(results_all_obj_smart, runtimes_all_obj_smart, \n",
    "               sol_dicts_all_obj_smart, results_ncflow, runtimes_ncflow,\n",
    "               results_cspf, runtimes_cspf)\n",
    "\n",
    "plot_benchmark(results_all_obj_smart_gen, runtimes_all_obj_smart_gen, \n",
    "               sol_dicts_all_obj_smart_gen, results_ncflow, runtimes_ncflow,\n",
    "               results_cspf, runtimes_cspf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#problem = Problem.from_file(\"../topologies/topology-zoo/GtsCe.graphml\", \n",
    "#                            \"../traffic-matrices/uniform/GtsCe.graphml_uniform_1475504323_64.0_0.05_traffic-matrix.pkl\")\n",
    "#problem.G\n",
    "#print(problem.G.edges.data())\n",
    "#com_list = problem.commodity_list\n",
    "#problem2 = problem.copy()\n",
    "#print(dir(problem))\n",
    "\n",
    "#pf = PathFormulation.new_max_flow(\n",
    "#                    num_paths,\n",
    "#                    edge_disjoint=edge_disjoint,\n",
    "#                    dist_metric=dist_metric)\n",
    "        \n",
    "#paths_dict = pf.get_paths(problem)\n",
    "#com0 = com_list[3]\n",
    "#print(com0)\n",
    "#print(paths_dict[(com0[1][0],com0[1][1])])\n",
    "\n",
    "\"\"\"\n",
    "problem2.traffic_matrix.tm\n",
    "new_tm = problem2.traffic_matrix.tm[0:10,:]\n",
    "\n",
    "num_rows = len(problem2.traffic_matrix.tm)\n",
    "\n",
    "shuffled_indices = list(range(num_rows))\n",
    "random.shuffle(shuffled_indices)\n",
    "\n",
    "num_first_problem = math.floor(num_rows/2)\n",
    "\n",
    "for i in shuffled_indices[1:num_first_problem]:\n",
    "    problem2.traffic_matrix.tm[i,:] = 0\n",
    "\n",
    "#print(problem2.traffic_matrix.tm[1:5,:])\n",
    "\n",
    "for u,v in problem.G.edges:\n",
    "    problem.G[u][v]['capacity'] = problem.G[u][v]['capacity']/2\n",
    "    problem2.G[u][v]['capacity'] = problem2.G[u][v]['capacity']/2\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CSPF(problems):\n",
    "    \n",
    "    results_all = []\n",
    "    runtimes_all = []\n",
    "    \n",
    "    for problem_name, topo_fname, tm_fname in problems:\n",
    "        problem = Problem.from_file(topo_fname, tm_fname)\n",
    "        \n",
    "        com_list = problem.commodity_list\n",
    "        tm = problem.traffic_matrix.tm\n",
    "        \n",
    "        pf = PathFormulation.new_max_flow(\n",
    "                    num_paths,\n",
    "                    edge_disjoint=edge_disjoint,\n",
    "                    dist_metric=dist_metric)\n",
    "        \n",
    "        paths_dict = pf.get_paths(problem)\n",
    "        \n",
    "        # initialize link capacity dict\n",
    "        remaining_link_capacity_dict = {}\n",
    "        for u,v in problem.G.edges:\n",
    "            remaining_link_capacity_dict[(u,v)] = problem.G[u][v]['capacity']\n",
    "            \n",
    "        # sort paths in ascending order\n",
    "        all_paths_list = []\n",
    "        allocated_coms = {}\n",
    "        for k, (source, target, demand) in com_list:\n",
    "            paths_array = paths_dict[(source, target)]\n",
    "            all_paths_list += paths_array\n",
    "            allocated_coms[(source,target)] = False\n",
    "        all_paths_list.sort(key=len)\n",
    "        \n",
    "        # iterate through sorted paths\n",
    "        total_allocated_flow = 0\n",
    "        startTime = datetime.now()\n",
    "        for path in all_paths_list:\n",
    "            source = path[0]\n",
    "            target = path[-1]\n",
    "            demand = tm[source,target]\n",
    "            \n",
    "            # skip if we have already allocated this commodity\n",
    "            if allocated_coms[(source,target)]:\n",
    "                continue\n",
    "            \n",
    "            # check that each edge in list has enough capacity\n",
    "            edge_list = list(path_to_edge_list(path))\n",
    "            room = True\n",
    "            for u,v in edge_list:\n",
    "                if remaining_link_capacity_dict[(u,v)] < demand:\n",
    "                    room = False\n",
    "                    break\n",
    "            \n",
    "            if not room:\n",
    "                continue\n",
    "            \n",
    "            # allocate\n",
    "            for u,v in edge_list:\n",
    "                remaining_link_capacity_dict[(u,v)] -= demand\n",
    "            allocated_coms[(source, target)] = True\n",
    "            total_allocated_flow += demand\n",
    "        runtime = datetime.now() - startTime\n",
    "        \n",
    "        results_all.append(total_allocated_flow)\n",
    "        runtimes_all.append(runtime.total_seconds())\n",
    "        print(\"Runtime: \" + str(runtime))\n",
    "        print(\"Allocated Flow:\" + str(total_allocated_flow))\n",
    "    return results_all, runtimes_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_paths, edge_disjoint, dist_metric = PATH_FORM_HYPERPARAMS\n",
    "results_cspf, runtimes_cspf = CSPF(problems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_ncflow)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
